# Reusable AI Code Review Workflow
# Usage: Âú®ÂÖ∂‰ªñÈ°πÁõÆ‰∏≠ÈÄöËøá workflow_call Ë∞ÉÁî®
#
# Example:
#   jobs:
#     review:
#       uses: your-org/CodeReview/.github/workflows/ai-code-review.yml@main
#       secrets:
#         AI_API_KEY: ${{ secrets.GEMINI_API_KEY }}

name: AI Code Review

on:
  workflow_call:
    inputs:
      ai-provider:
        description: 'AI provider (gemini, openai, claude)'
        required: false
        type: string
        default: 'gemini'
      ai-model:
        description: 'AI model name (empty for provider default)'
        required: false
        type: string
        default: ''
      min-score:
        description: 'Minimum score for auto-approval (0-10)'
        required: false
        type: number
        default: 7
      max-files:
        description: 'Max files changed for auto-merge'
        required: false
        type: number
        default: 20
      max-lines:
        description: 'Max lines changed for auto-merge'
        required: false
        type: number
        default: 500
      auto-merge:
        description: 'Enable auto-merge for approved PRs'
        required: false
        type: boolean
        default: true
      merge-method:
        description: 'Merge method (merge, squash, rebase)'
        required: false
        type: string
        default: 'squash'
      language:
        description: 'Review language (zh, en)'
        required: false
        type: string
        default: 'zh'
      run-security-scan:
        description: 'Run security vulnerability scan with Trivy'
        required: false
        type: boolean
        default: true
      exclude-patterns:
        description: 'File patterns to exclude from review (comma-separated)'
        required: false
        type: string
        default: '*.md,*.txt,*.json,*.lock,*.yaml,*.yml'
      custom-prompt:
        description: 'Additional review instructions'
        required: false
        type: string
        default: ''
    secrets:
      AI_API_KEY:
        description: 'AI API Key (Gemini/OpenAI/Claude)'
        required: true
    outputs:
      score:
        description: 'AI review score (0-10)'
        value: ${{ jobs.ai-code-review.outputs.score }}
      approved:
        description: 'Whether PR is approved'
        value: ${{ jobs.ai-code-review.outputs.approved }}
      merged:
        description: 'Whether PR was auto-merged'
        value: ${{ jobs.auto-approve-and-merge.outputs.merged }}

permissions:
  contents: write
  pull-requests: write
  checks: write
  security-events: read

jobs:
  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    if: ${{ inputs.run-security-scan }}
    timeout-minutes: 10
    outputs:
      trivy-status: ${{ steps.trivy.outcome }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        id: trivy
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'json'
          output: 'trivy-report.json'
          severity: 'CRITICAL,HIGH,MEDIUM'
        continue-on-error: true

      - name: Upload security report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: trivy-report.json

  ai-code-review:
    name: AI Code Review
    runs-on: ubuntu-latest
    needs: [security-scan]
    timeout-minutes: 15
    if: |
      always() &&
      (needs.security-scan.result == 'success' || needs.security-scan.result == 'skipped')
    outputs:
      score: ${{ steps.ai-review.outputs.score }}
      approved: ${{ steps.ai-review.outputs.approved }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get PR diff
        id: diff
        run: |
          git fetch origin ${{ github.base_ref }}

          # Build exclude patterns for git diff
          EXCLUDE_ARGS=""
          IFS=',' read -ra PATTERNS <<< "${{ inputs.exclude-patterns }}"
          for pattern in "${PATTERNS[@]}"; do
            pattern=$(echo "$pattern" | xargs)  # trim whitespace
            EXCLUDE_ARGS="$EXCLUDE_ARGS -- ':(exclude)$pattern'"
          done

          # Also exclude sensitive files
          EXCLUDE_ARGS="$EXCLUDE_ARGS -- ':(exclude).env*' -- ':(exclude)*secret*' -- ':(exclude)*.pem' -- ':(exclude)*.key'"

          # Get diff excluding patterns
          eval "DIFF=\$(git diff origin/${{ github.base_ref }}...HEAD $EXCLUDE_ARGS)" || DIFF=""

          if [ -z "$DIFF" ] || [ $(echo "$DIFF" | wc -l) -lt 3 ]; then
            echo "empty_diff=true" >> $GITHUB_OUTPUT
            echo "No code changes detected (only docs/config/excluded files)"
          else
            echo "$DIFF" > pr-diff.txt
            LINES_CHANGED=$(echo "$DIFF" | wc -l)
            echo "lines_changed=$LINES_CHANGED" >> $GITHUB_OUTPUT
            echo "empty_diff=false" >> $GITHUB_OUTPUT
          fi

      - name: Auto approve docs-only PR
        if: steps.diff.outputs.empty_diff == 'true'
        id: auto-approve-docs
        run: |
          echo "score=10" >> $GITHUB_OUTPUT
          echo "approved=true" >> $GITHUB_OUTPUT
          cat > ai-review-report.json << 'DOCEOF'
          {
            "overall_score": 10,
            "quality": 10,
            "security": 10,
            "performance": 10,
            "testing": 10,
            "best_practices": 10,
            "approved": true,
            "summary": "Documentation/config only changes, auto-approved",
            "issues": [],
            "suggestions": []
          }
          DOCEOF

      - name: Install AI SDK
        if: steps.diff.outputs.empty_diff != 'true'
        run: |
          if [ "${{ inputs.ai-provider }}" = "gemini" ]; then
            pip install google-generativeai
          elif [ "${{ inputs.ai-provider }}" = "openai" ]; then
            pip install openai
          elif [ "${{ inputs.ai-provider }}" = "claude" ]; then
            pip install anthropic
          fi

      - name: AI Code Review
        id: ai-review
        if: steps.diff.outputs.empty_diff != 'true'
        env:
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
          AI_PROVIDER: ${{ inputs.ai-provider }}
          AI_MODEL: ${{ inputs.ai-model }}
          MIN_SCORE: ${{ inputs.min-score }}
          REVIEW_LANGUAGE: ${{ inputs.language }}
          CUSTOM_PROMPT: ${{ inputs.custom-prompt }}
          LINES_CHANGED: ${{ steps.diff.outputs.lines_changed }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import sys
          import time

          # Configuration
          api_key = os.environ.get('AI_API_KEY')
          provider = os.environ.get('AI_PROVIDER', 'gemini')
          model = os.environ.get('AI_MODEL', '')
          min_score = int(os.environ.get('MIN_SCORE', '7'))
          language = os.environ.get('REVIEW_LANGUAGE', 'zh')
          custom_prompt = os.environ.get('CUSTOM_PROMPT', '')
          lines_changed = os.environ.get('LINES_CHANGED', '0')

          # Default models per provider
          DEFAULT_MODELS = {
              'gemini': 'gemini-2.0-flash',
              'openai': 'gpt-4o',
              'claude': 'claude-sonnet-4-20250514'
          }
          model_name = model or DEFAULT_MODELS.get(provider, 'gemini-2.0-flash')

          # Read diff
          with open('pr-diff.txt', 'r') as f:
              diff = f.read()

          # Truncate if too large
          if len(diff) > 200000:
              diff = diff[:200000] + "\n... (truncated)"

          # Build prompt
          if language == 'zh':
              base_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ‰ª£Á†ÅÂÆ°Êü•‰∏ìÂÆ∂„ÄÇËØ∑ÂÆ°Êü•‰ª•‰∏ã Pull Request ÁöÑ‰ª£Á†ÅÂèòÊõ¥Ôºö

          {diff}

          ËØ∑‰ªé‰ª•‰∏ãÁª¥Â∫¶ËØÑ‰º∞ÔºàÊØèÈ°π 0-10 ÂàÜÔºâÔºö
          1. ‰ª£Á†ÅË¥®ÈáèÔºàÂèØËØªÊÄß„ÄÅÂ§çÊùÇÂ∫¶„ÄÅÂëΩÂêçÔºâ
          2. ÂÆâÂÖ®ÊÄßÔºàÊΩúÂú®ÊºèÊ¥û„ÄÅËæìÂÖ•È™åËØÅÔºâ
          3. ÊÄßËÉΩÔºàÁÆóÊ≥ïÊïàÁéá„ÄÅËµÑÊ∫ê‰ΩøÁî®Ôºâ
          4. ÊµãËØïË¶ÜÁõñÔºàÊòØÂê¶ÊúâÁõ∏Â∫îÊµãËØïÔºâ
          5. ÊúÄ‰Ω≥ÂÆûË∑µÔºàÊòØÂê¶ÈÅµÂæ™È°πÁõÆËßÑËåÉÔºâ

          {f"È¢ùÂ§ñË¶ÅÊ±ÇÔºö{custom_prompt}" if custom_prompt else ""}

          ËøîÂõû JSON Ê†ºÂºèÔºàÂè™ËøîÂõû JSONÔºå‰∏çË¶ÅÂÖ∂‰ªñÂÜÖÂÆπÔºâÔºö
          {{
            "overall_score": ÊÄªÂàÜÔºà0-10ÔºåÊï∞Â≠óÁ±ªÂûãÔºâÔºå
            "quality": ÂàÜÊï∞ÔºàÊï∞Â≠óÁ±ªÂûãÔºâ,
            "security": ÂàÜÊï∞ÔºàÊï∞Â≠óÁ±ªÂûãÔºâ,
            "performance": ÂàÜÊï∞ÔºàÊï∞Â≠óÁ±ªÂûãÔºâ,
            "testing": ÂàÜÊï∞ÔºàÊï∞Â≠óÁ±ªÂûãÔºâ,
            "best_practices": ÂàÜÊï∞ÔºàÊï∞Â≠óÁ±ªÂûãÔºâ,
            "approved": true/falseÔºàÂ∏ÉÂ∞îÁ±ªÂûãÔºåÊÄªÂàÜ>={min_score}ÂàôÊâπÂáÜÔºâ,
            "summary": "ÁÆÄÁü≠ÊÄªÁªìÔºà‰∏≠ÊñáÂ≠óÁ¨¶‰∏≤Ôºâ",
            "issues": ["ÈóÆÈ¢ò1", "ÈóÆÈ¢ò2", ...]ÔºàÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÔºâ,
            "suggestions": ["Âª∫ËÆÆ1", "Âª∫ËÆÆ2", ...]ÔºàÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÔºâ
          }}
          """
          else:
              base_prompt = f"""You are a professional code review expert. Please review the following Pull Request changes:

          {diff}

          Evaluate from these dimensions (0-10 for each):
          1. Code Quality (readability, complexity, naming)
          2. Security (potential vulnerabilities, input validation)
          3. Performance (algorithm efficiency, resource usage)
          4. Test Coverage (whether there are corresponding tests)
          5. Best Practices (following project conventions)

          {f"Additional requirements: {custom_prompt}" if custom_prompt else ""}

          Return JSON format (only JSON, no other content):
          {{
            "overall_score": total score (0-10, number),
            "quality": score (number),
            "security": score (number),
            "performance": score (number),
            "testing": score (number),
            "best_practices": score (number),
            "approved": true/false (boolean, approve if score >= {min_score}),
            "summary": "brief summary (string)",
            "issues": ["issue1", "issue2", ...] (string array),
            "suggestions": ["suggestion1", "suggestion2", ...] (string array)
          }}
          """

          def validate_scores(result):
              """Ensure all scores are within 0-10 range"""
              for key in ['overall_score', 'quality', 'security', 'performance', 'testing', 'best_practices']:
                  if key in result:
                      try:
                          result[key] = max(0, min(10, float(result[key])))
                      except (ValueError, TypeError):
                          result[key] = 0
              return result

          def call_ai_with_retry(prompt, max_retries=3):
              """Call AI API with exponential backoff retry"""
              start_time = time.time()

              for attempt in range(max_retries):
                  try:
                      if provider == 'gemini':
                          import google.generativeai as genai
                          genai.configure(api_key=api_key)
                          model_obj = genai.GenerativeModel(
                              model_name=model_name,
                              generation_config={
                                  'temperature': 0.3,
                                  'top_p': 0.95,
                                  'top_k': 40,
                                  'max_output_tokens': 8192,
                                  'response_mime_type': 'application/json',
                              }
                          )
                          response = model_obj.generate_content(prompt)
                          return response.text, time.time() - start_time

                      elif provider == 'openai':
                          from openai import OpenAI
                          client = OpenAI(api_key=api_key)
                          response = client.chat.completions.create(
                              model=model_name,
                              messages=[{"role": "user", "content": prompt}],
                              temperature=0.3,
                              max_tokens=8192,
                              response_format={"type": "json_object"}
                          )
                          return response.choices[0].message.content, time.time() - start_time

                      elif provider == 'claude':
                          import anthropic
                          client = anthropic.Anthropic(api_key=api_key)
                          response = client.messages.create(
                              model=model_name,
                              max_tokens=8192,
                              messages=[{"role": "user", "content": prompt}]
                          )
                          return response.content[0].text, time.time() - start_time

                  except Exception as e:
                      if attempt < max_retries - 1:
                          wait_time = 2 ** attempt
                          print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s: {e}")
                          time.sleep(wait_time)
                      else:
                          raise

          try:
              response_text, elapsed_time = call_ai_with_retry(base_prompt)
              result = json.loads(response_text)
              result = validate_scores(result)

              # Add metadata
              result['_metadata'] = {
                  'model': model_name,
                  'provider': provider,
                  'elapsed_seconds': round(elapsed_time, 2),
                  'lines_changed': int(lines_changed)
              }

              print(f"AI Review Score: {result['overall_score']}/10")
              print(f"Approved: {result['approved']}")
              print(f"Model: {model_name} ({provider})")
              print(f"Time: {elapsed_time:.2f}s")

              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"score={result['overall_score']}\n")
                  f.write(f"approved={str(result['approved']).lower()}\n")

              with open('ai-review-report.json', 'w') as f:
                  json.dump(result, f, indent=2, ensure_ascii=False)

          except json.JSONDecodeError as e:
              print(f"JSON parsing failed: {e}")
              print(f"Raw response: {response_text[:1000]}")
              try:
                  import re
                  json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                  if json_match:
                      result = json.loads(json_match.group())
                      result = validate_scores(result)
                      with open('ai-review-report.json', 'w') as f:
                          json.dump(result, f, indent=2, ensure_ascii=False)
                      with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                          f.write(f"score={result['overall_score']}\n")
                          f.write(f"approved={str(result['approved']).lower()}\n")
                  else:
                      raise
              except:
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write("score=0\n")
                      f.write("approved=false\n")
                  sys.exit(1)
          except Exception as e:
              print(f"AI Review failed: {e}")
              import traceback
              traceback.print_exc()
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("score=0\n")
                  f.write("approved=false\n")
              sys.exit(1)
          EOF

      - name: Upload AI review report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-review-report
          path: ai-review-report.json

      - name: Comment AI review on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let report;
            try {
              report = JSON.parse(fs.readFileSync('ai-review-report.json', 'utf8'));
            } catch (e) {
              console.log('No review report found');
              return;
            }

            const lang = '${{ inputs.language }}';
            const labels = lang === 'zh' ? {
              title: 'AI ‰ª£Á†ÅÂÆ°Êü•Êä•Âëä',
              score: 'ÁªºÂêàËØÑÂàÜ',
              details: 'ËØ¶ÁªÜËØÑÂàÜ',
              quality: '‰ª£Á†ÅË¥®Èáè',
              security: 'ÂÆâÂÖ®ÊÄß',
              performance: 'ÊÄßËÉΩ',
              testing: 'ÊµãËØïË¶ÜÁõñ',
              bestPractices: 'ÊúÄ‰Ω≥ÂÆûË∑µ',
              summary: 'ÊÄªÁªì',
              issues: 'ÂèëÁé∞ÁöÑÈóÆÈ¢ò',
              suggestions: 'Âª∫ËÆÆ',
              approved: '**Ê≠§ PR Â∑≤ÈÄöËøá AI ÂÆ°Êü•ÔºåÂèØ‰ª•Ëá™Âä®ÂêàÂπ∂**',
              needsReview: '**Ê≠§ PR ÈúÄË¶Å‰∫∫Â∑•ÂÆ°Êü•**'
            } : {
              title: 'AI Code Review Report',
              score: 'Overall Score',
              details: 'Detailed Scores',
              quality: 'Code Quality',
              security: 'Security',
              performance: 'Performance',
              testing: 'Testing',
              bestPractices: 'Best Practices',
              summary: 'Summary',
              issues: 'Issues Found',
              suggestions: 'Suggestions',
              approved: '**This PR is approved by AI and ready for auto-merge**',
              needsReview: '**This PR needs manual review**'
            };

            // Build metadata line
            const meta = report._metadata || {};
            const metaLine = `Model: ${meta.model || 'unknown'} | Time: ${meta.elapsed_seconds || '?'}s | Lines: ${meta.lines_changed || '?'}`;

            const body = `## ${labels.title}

**${labels.score}:** ${report.overall_score}/10 ${report.approved ? '‚úÖ' : '‚ö†Ô∏è'}

### ${labels.details}
| Dimension | Score |
|-----------|-------|
| ${labels.quality} | ${report.quality}/10 |
| ${labels.security} | ${report.security}/10 |
| ${labels.performance} | ${report.performance}/10 |
| ${labels.testing} | ${report.testing}/10 |
| ${labels.bestPractices} | ${report.best_practices}/10 |

### ${labels.summary}
${report.summary}

${report.issues && report.issues.length > 0 ? `### ‚ö†Ô∏è ${labels.issues}\n${report.issues.map(i => '- ' + i).join('\n')}` : ''}

${report.suggestions && report.suggestions.length > 0 ? `### üí° ${labels.suggestions}\n${report.suggestions.map(s => '- ' + s).join('\n')}` : ''}

---
${report.approved ? '‚úÖ ' + labels.approved : '‚ö†Ô∏è ' + labels.needsReview}

<sub>üìä ${metaLine}</sub>`;

            // Find existing AI review comment and update it
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const existingComment = comments.data.find(c =>
              c.body.includes('AI') &&
              c.body.includes(labels.title) &&
              c.user.type === 'Bot'
            );

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: body
              });
              console.log('Updated existing comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
              console.log('Created new comment');
            }

  auto-approve-and-merge:
    name: Auto Approve and Merge
    runs-on: ubuntu-latest
    needs: [security-scan, ai-code-review]
    timeout-minutes: 5
    if: |
      always() &&
      inputs.auto-merge &&
      needs.ai-code-review.outputs.approved == 'true'
    outputs:
      merged: ${{ steps.merge.outputs.merged }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check PR size
        id: pr-size
        env:
          MAX_FILES: ${{ inputs.max-files }}
          MAX_LINES: ${{ inputs.max-lines }}
        run: |
          git fetch origin ${{ github.base_ref }}
          FILES_CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | wc -l)
          LINES_CHANGED=$(git diff origin/${{ github.base_ref }}...HEAD | wc -l)

          echo "files_changed=$FILES_CHANGED" >> $GITHUB_OUTPUT
          echo "lines_changed=$LINES_CHANGED" >> $GITHUB_OUTPUT

          if [ $FILES_CHANGED -gt $MAX_FILES ] || [ $LINES_CHANGED -gt $MAX_LINES ]; then
            echo "too_large=true" >> $GITHUB_OUTPUT
            echo "PR too large for auto-merge: $FILES_CHANGED files, $LINES_CHANGED lines"
          else
            echo "too_large=false" >> $GITHUB_OUTPUT
          fi

      - name: Auto approve PR
        if: steps.pr-size.outputs.too_large == 'false'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            await github.rest.pulls.createReview({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number,
              event: 'APPROVE',
              body: '‚úÖ Auto-approved by AI code review system\n\n- AI review score: ${{ needs.ai-code-review.outputs.score }}/10\n- Security scan: ${{ needs.security-scan.result }}'
            });

      - name: Merge PR
        id: merge
        if: steps.pr-size.outputs.too_large == 'false'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.pulls.merge({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.issue.number,
                merge_method: '${{ inputs.merge-method }}',
                commit_title: `${{ github.event.pull_request.title }} (#${{ github.event.pull_request.number }})`,
                commit_message: 'Auto-merged by AI code review system'
              });
              core.setOutput('merged', 'true');
            } catch (e) {
              console.log('Merge failed:', e.message);
              core.setOutput('merged', 'false');
            }

      - name: Comment merge status
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const tooLarge = '${{ steps.pr-size.outputs.too_large }}' === 'true';
            const merged = '${{ steps.merge.outputs.merged }}' === 'true';
            const lang = '${{ inputs.language }}';

            let body;
            if (tooLarge) {
              body = lang === 'zh'
                ? `‚ö†Ô∏è **Ëá™Âä®ÂêàÂπ∂Â∑≤Ë∑≥Ëøá**: PR ÂèòÊõ¥ËøáÂ§ß (${{ steps.pr-size.outputs.files_changed }} Êñá‰ª∂, ${{ steps.pr-size.outputs.lines_changed }} Ë°å)„ÄÇËØ∑Áî≥ËØ∑‰∫∫Â∑•ÂÆ°Êü•„ÄÇ`
                : `‚ö†Ô∏è **Auto-merge skipped**: PR is too large (${{ steps.pr-size.outputs.files_changed }} files, ${{ steps.pr-size.outputs.lines_changed }} lines). Please request manual review.`;
            } else if (merged) {
              body = lang === 'zh'
                ? '‚úÖ **PR Â∑≤Ëá™Âä®ÂêàÂπ∂ÊàêÂäüÔºÅ**'
                : '‚úÖ **PR auto-merged successfully!**';
            } else {
              body = lang === 'zh'
                ? '‚ö†Ô∏è **Ëá™Âä®ÂêàÂπ∂Â§±Ë¥•ÔºåËØ∑‰∫∫Â∑•Â§ÑÁêÜ**'
                : '‚ö†Ô∏è **Auto-merge failed, please merge manually**';
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
